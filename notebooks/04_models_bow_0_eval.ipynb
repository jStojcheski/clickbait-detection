{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models: Bag of Words - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set input data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = '../data/03_preprocessed_01_non_letters_to_empty_space'\n",
    "\n",
    "pprint(sorted(os.listdir(data_in)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = '../data/04_models_bag_of_words'\n",
    "# pprint(sorted(os.listdir(data_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data into data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clck = pd.read_csv(f'{data_in}/clck.csv', header=None)\n",
    "clck.columns = ['_headlines']\n",
    "clck['clickbait'] = pd.Series(np.ones(len(clck)) == 1)\n",
    "clck[0:len(clck):5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(f'{data_in}/news.csv', header=None)\n",
    "news.columns = ['_headlines']\n",
    "news['clickbait'] = pd.Series(np.ones(len(news)) == 0)\n",
    "news[0:len(news):5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([clck, news], ignore_index=True)\n",
    "df[0:len(df):5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{data_out}/../data.csv'):\n",
    "    df = pd.read_csv(f'{data_out}/../data.csv', index_col='Unnamed: 0')\n",
    "    df = df.fillna('<nan>')\n",
    "    \n",
    "else:\n",
    "    for regex in [None]:\n",
    "        for stop_words in [None, set(stopwords.words('english'))]:\n",
    "            for lemmatizer in [None, WordNetLemmatizer()]:\n",
    "                for stemmer in [None, PorterStemmer()]:\n",
    "\n",
    "                    settings  = ''\n",
    "                    settings += '' if regex is None else '_re'\n",
    "                    settings += '' if stop_words is None else '_sw'\n",
    "                    settings += '' if lemmatizer is None else '_lm'\n",
    "                    settings += '' if stemmer is None else '_sm'\n",
    "\n",
    "                    print(f'headlines{settings:20s} ...', end=' ')\n",
    "\n",
    "                    t = time.time()\n",
    "                    headlines = list()\n",
    "\n",
    "                    for i in range(len(df)):\n",
    "                        # if (i+1) % 1000 == 0:\n",
    "                        # print(f'Review {i+1:>6d} of {len(df):>6d}')\n",
    "\n",
    "                        text = df['_headlines'][i]\n",
    "\n",
    "                        # split the headline into words\n",
    "                        text = re.split('\\s+', text.lower().strip())\n",
    "\n",
    "                        # replace unnecessary patterns\n",
    "                        if regex is not None:\n",
    "                            text = regex.sub(' ', text)\n",
    "\n",
    "                        # remove stop-words\n",
    "                        if stop_words is not None:\n",
    "                            text = [w for w in text if w not in stop_words]\n",
    "\n",
    "                        # lemmatization\n",
    "                        if lemmatizer is not None:\n",
    "                            text = [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "                        # stemming\n",
    "                        if stemmer is not None:\n",
    "                            text = [stemmer.stem(w) for w in text]\n",
    "\n",
    "                        headlines.append(' '.join(text))\n",
    "\n",
    "                    df[f'headlines{settings}'] = pd.Series(headlines)\n",
    "\n",
    "                    print(f'Done in {int(np.ceil(time.time() - t))} seconds!')\n",
    "\n",
    "    df.to_csv(f'{data_out}/../data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=24101989)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dfs():\n",
    "    acc_df = pd.DataFrame(index=[i+1 for i in range(kf.n_splits)])\n",
    "    out_df = pd.DataFrame(columns=['i'], index=df.index)\n",
    "    tim_df = pd.DataFrame(index=[i+1 for i in range(kf.n_splits)])\n",
    "\n",
    "    i = 1\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        out_df['i'][test_index] = i\n",
    "        i += 1\n",
    "        \n",
    "    return acc_df, out_df, tim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_series():\n",
    "    acc_s = pd.Series(data=[np.nan for _ in range(kf.n_splits)],\n",
    "                      index=acc_df.index)\n",
    "    out_s = pd.Series(data=[np.nan for _ in range(len(df))],\n",
    "                      index=out_df.index)\n",
    "    tim_s = pd.Series(data=[np.nan for _ in range(kf.n_splits)],\n",
    "                      index=tim_df.index)\n",
    "\n",
    "    return acc_s, out_s, tim_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(cl, cl_name):\n",
    "    acc_s, out_s, tim_s = init_series()\n",
    "    \n",
    "    i = 1\n",
    "    for train_index, test_index in kf.split(df):\n",
    "\n",
    "        # initialize count vectorizer\n",
    "        vectorizer = CountVectorizer(analyzer='word',\n",
    "                                     max_features=ft)\n",
    "\n",
    "        t = time.time()\n",
    "\n",
    "        train_x = df[f'headlines{setting}'][train_index]\n",
    "        train_y = df['clickbait'][train_index]\n",
    "        test_x  = df[f'headlines{setting}'][test_index]\n",
    "        test_y  = df['clickbait'][test_index]\n",
    "\n",
    "        train_ft = vectorizer.fit_transform(train_x)\n",
    "        train_ft = train_ft.toarray()\n",
    "\n",
    "        test_ft = vectorizer.transform(test_x)\n",
    "        test_ft = test_ft.toarray()\n",
    "\n",
    "        # fit & predict\n",
    "        cl = cl.fit(train_ft, train_y)\n",
    "        results = pd.Series(cl.predict(test_ft),\n",
    "                            index=test_y.index)\n",
    "\n",
    "        # save results of the i-th split\n",
    "        acc_s[i]            = np.sum(results == test_y) / len(test_y)\n",
    "        out_s[test_y.index] = results\n",
    "        tim_s[i]            = np.around(time.time() - t, decimals=3)\n",
    "        print(f'{cl_name:30s} [{i:>2d}] | {(acc_s[i] * 100):>6.3f} % | {int(np.ceil(tim_s[i])):>4d} s')\n",
    "        i += 1\n",
    "\n",
    "    return acc_s, out_s, tim_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cl, cl_name):\n",
    "    acc_s, out_s, tim_s = fit_and_predict(cl, cl_name)\n",
    "    acc_df[cl_name] = acc_s\n",
    "    out_df[cl_name] = out_s\n",
    "    tim_df[cl_name] = tim_s\n",
    "    \n",
    "    print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit & Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings of interest\n",
    "soi = [\n",
    "       'headlines',\n",
    "       'headlines_sw',\n",
    "       'headlines_lm',\n",
    "       'headlines_sm',\n",
    "       'headlines_sw_lm',\n",
    "       'headlines_sw_sm',\n",
    "       'headlines_lm_sm',\n",
    "       'headlines_sw_lm_sm',\n",
    "      ]\n",
    "\n",
    "acc_df, out_df, tim_df = init_dfs()\n",
    "\n",
    "now = ''.join([f'{item:02d}' for item in time.localtime()[:6]])\n",
    "\n",
    "for s in soi:\n",
    "    setting = s[9:]\n",
    "    \n",
    "    for ft in [1000, 2500, 5000]:\n",
    "        cycle_time = time.time()\n",
    "\n",
    "        # Extra Trees\n",
    "        for est in [100]:\n",
    "            run(ExtraTreesClassifier(n_estimators=est, n_jobs=-1,\n",
    "                                     verbose=True, bootstrap=True),\n",
    "                f'{ft}f{setting}_ET_{est}est')\n",
    "\n",
    "        # Logistic Regression\n",
    "        for c in [1.00]:\n",
    "            run(LogisticRegression(C=c, max_iter=1000,\n",
    "                                   n_jobs=-1, verbose=True),\n",
    "                f'{ft}f{setting}_LogReg_{c:.2f}C')\n",
    "\n",
    "        # Naïve Bayes\n",
    "        for alpha in [1.00]:\n",
    "            run(BernoulliNB(alpha=alpha), \n",
    "                f'{ft}f{setting}_NB_{alpha:.2f}alpha')\n",
    "            \n",
    "        # Neural Network\n",
    "        for max_iter in [1000]:\n",
    "            run(MLPClassifier(max_iter=max_iter, verbose=True),\n",
    "                f'{ft}f{setting}_NN_{max_iter}iter')\n",
    "            \n",
    "        # Random Forests\n",
    "        for est in [100]:\n",
    "            run(RandomForestClassifier(n_estimators=est, n_jobs=-1,\n",
    "                                       verbose=True, bootstrap=True),\n",
    "                f'{ft}f{setting}_RF_{est}e')\n",
    "            \n",
    "        print(f'\\n========== {ft}f{setting} cycle finished in {int(np.ceil(time.time() - cycle_time))} seconds. ==========\\n\\n\\n')\n",
    "\n",
    "acc_df = acc_df.transpose()\n",
    "tim_df = tim_df.transpose()\n",
    "\n",
    "acc_df.to_csv(f'{data_out}/acc/{now}.csv')\n",
    "out_df.to_csv(f'{data_out}/out/{now}.csv')\n",
    "tim_df.to_csv(f'{data_out}/tim/{now}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings of interest\n",
    "soi = [\n",
    "       'headlines',\n",
    "       'headlines_lm',\n",
    "       'headlines_sm',\n",
    "       'headlines_lm_sm',\n",
    "      ]\n",
    "\n",
    "acc_df, out_df, tim_df = init_dfs()\n",
    "\n",
    "now = ''.join([f'{item:02d}' for item in time.localtime()[:6]])\n",
    "\n",
    "for s in soi:\n",
    "    setting = s[9:]\n",
    "    \n",
    "    for ft in [5000]:\n",
    "        cycle_time = time.time()\n",
    "\n",
    "        # Logistic Regression\n",
    "        for c in [0.0001, 0.25, 0.50, 0.75, 1.00, 1.25, 1.50]:\n",
    "            run(LogisticRegression(C=c, max_iter=1000,\n",
    "                                   n_jobs=-1, verbose=True),\n",
    "                f'{ft}f{setting}_LogReg_{c:.2f}C')\n",
    "\n",
    "        # Naïve Bayes\n",
    "        for alpha in [0.00, 0.25, 0.50, 0.75, 1.00, 1.25, 1.50]:\n",
    "            run(BernoulliNB(alpha=alpha), \n",
    "                f'{ft}f{setting}_NB_{alpha:.2f}alpha')\n",
    "            \n",
    "        print(f'\\n========== {ft}f{setting} cycle finished in {int(np.ceil(time.time() - cycle_time))} seconds. ==========\\n\\n\\n')\n",
    "\n",
    "acc_df = acc_df.transpose()\n",
    "tim_df = tim_df.transpose()\n",
    "\n",
    "acc_df.to_csv(f'{data_out}/acc/{now}.csv')\n",
    "out_df.to_csv(f'{data_out}/out/{now}.csv')\n",
    "tim_df.to_csv(f'{data_out}/tim/{now}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
